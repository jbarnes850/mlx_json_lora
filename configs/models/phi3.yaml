# Phi-3.5 configuration
model:
  name: "microsoft/Phi-3.5-mini-instruct"
  path: "microsoft/Phi-3.5-mini-instruct"
  batch_size: 1
  max_seq_length: 2048
  learning_rate: 2.0e-4
  num_layers: 32

# Training parameters
training:
  seed: 42
  iters: 600
  val_batches: 20
  steps_per_report: 10
  steps_per_eval: 50
  save_every: 100
  grad_checkpoint: true

# LoRA configuration for Phi-3.5
lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules:
    - "self_attn.q_proj"
    - "self_attn.k_proj"
    - "self_attn.v_proj"
    - "self_attn.o_proj"
    - "mlp.gate_proj"
    - "mlp.up_proj"
    - "mlp.down_proj"
  lora_layers: 32
